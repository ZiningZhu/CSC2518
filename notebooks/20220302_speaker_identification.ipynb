{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from pathlib import Path \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torchaudio \n",
    "from speechbrain.dataio.preprocess import AudioNormalizer\n",
    "import librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    signal, sr = torchaudio.load(path, channels_first=False)\n",
    "    return AudioNormalizer()(signal, sr)\n",
    "\n",
    "CLIP_TIME = 10  # Use this length of audio. CLIP_TIME=10 means 10 seconds.\n",
    "SAMPLE_RATE = 16000\n",
    "base_dir = \"../data/LibriSpeech\"\n",
    "\n",
    "def get_audio_signals(split_name=\"dev-clean\", setting=\"one_utterance_per_speaker\"):\n",
    "    all_utterances = []\n",
    "    for speaker in Path(base_dir, split_name).iterdir():\n",
    "        speaker_collected = False \n",
    "        if not speaker.is_dir():\n",
    "            continue \n",
    "\n",
    "        for session in speaker.iterdir():\n",
    "            if speaker_collected:\n",
    "                break \n",
    "            if not session.is_dir():\n",
    "                continue \n",
    "\n",
    "            for utt in session.iterdir():\n",
    "                if not str(utt).endswith(\"flac\"):\n",
    "                    continue \n",
    "\n",
    "                signal, sr = torchaudio.load(str(utt), channels_first=False)\n",
    "                assert sr == SAMPLE_RATE, f\"Expected sample rate is {SAMPLE_RATE}. Got {sr}\"\n",
    "                audio_len = len(signal) / sr \n",
    "                if audio_len > CLIP_TIME:\n",
    "                    s = signal[: CLIP_TIME * sr, 0]\n",
    "                    all_utterances.append(s)\n",
    "                    speaker_collected = True \n",
    "                    break\n",
    "\n",
    "    return torch.stack(all_utterances)\n",
    "\n",
    "dev_speeches = get_audio_signals(\"dev-clean\")  # (N_speaker, CLIP_TIME * SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(speeches):\n",
    "    mfccs = []\n",
    "    for sig in speeches:\n",
    "        y = np.array(sig)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc=20)  # (n_mfcc, n_frame)\n",
    "        mfccs.append(mfcc)\n",
    "    return np.array(mfccs) \n",
    "\n",
    "dev_mfccs = preprocess_data(dev_speeches)  # (N_speaker, n_mfcc, n_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long is the total audio?  \n",
    "There are 250+40+40 speakers (train/dev/test folder), totaling 330 speakers. If we take 10s for each speaker, then total audio is 3300s -- might take up to 1 hour in transcription. For coding and debugging, just use dev set (400s) for now.  \n",
    "Also: Use the 0-5 seconds as train; 6-10 as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do speaker identification?  \n",
    "- Attempt 1: Concatenate all MFCC of a speaker. Run LogReg. Accuracy = 0.20  \n",
    "- Attempt 2: Use one frame of MFCC. Acc = 0.51 (LogReg), 0.54 (MLPClassifier)  \n",
    "- Attempt 3: Concatenate some frames of MFCC, and use the rest as data samples. The highest acc is 0.54; not significantly better than using one frame of MFCC.  \n",
    "- Attempt 4: Concatenate some frames of MFCC. Use LSTM. Not good; resulted in acc of around 0.22. Probably smaller batches can be more useful.  \n",
    "\n",
    "I'm going to proceed with using just one frame of MFCC henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "def speaker_identification_experiment1(train_speeches, val_speeches):\n",
    "    N_speaker = len(train_speeches)\n",
    "    labels = [i for i in range(N_speaker)]\n",
    "    train_feats = preprocess_data(train_speeches).reshape(N_speaker, -1)  \n",
    "    valid_feats = preprocess_data(val_speeches).reshape(N_speaker, -1)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_feats, labels)\n",
    "    pred = model.predict(valid_feats)\n",
    "    print(accuracy_score(labels, pred))\n",
    "\n",
    "speaker_identification_experiment1(\n",
    "    dev_speeches[:, :5*SAMPLE_RATE],\n",
    "    dev_speeches[:, 5*SAMPLE_RATE:])  # 1.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5415605095541401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzi/miniforge3/envs/transformers4/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def speaker_identification_experiment2(train_speeches, val_speeches):\n",
    "    \n",
    "    train_feats = preprocess_data(train_speeches)  # (n_speaker, n_mfcc, n_frame)\n",
    "    n_speaker = train_feats.shape[0]\n",
    "    n_mfcc = train_feats.shape[1]\n",
    "    n_frame = train_feats.shape[2]\n",
    "    train_X = np.swapaxes(train_feats, 1, 2).reshape(-1, n_mfcc)\n",
    "\n",
    "    valid_feats = preprocess_data(val_speeches)\n",
    "    valid_X = np.swapaxes(valid_feats, 1, 2).reshape(-1, n_mfcc)\n",
    "\n",
    "    labels = [speaker for speaker in range(n_speaker) for fr in range(n_frame) ]\n",
    "\n",
    "    model = MLPClassifier()\n",
    "    model.fit(train_X, labels)\n",
    "    pred = model.predict(valid_X)\n",
    "    print(accuracy_score(labels, pred))\n",
    "\n",
    "speaker_identification_experiment2(\n",
    "    dev_speeches[:, :5*SAMPLE_RATE],\n",
    "    dev_speeches[:, 5*SAMPLE_RATE:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5455\n"
     ]
    }
   ],
   "source": [
    "def speaker_identification_experiment3(train_speeches, val_speeches):\n",
    "    \n",
    "    train_feats = preprocess_data(train_speeches)  # (n_speaker, n_mfcc, n_frame)\n",
    "    n_speaker = train_feats.shape[0]\n",
    "    n_mfcc = train_feats.shape[1]\n",
    "    n_frame = train_feats.shape[2]\n",
    "    n_per_sample = 3\n",
    "    n_sample_per_speaker = 50\n",
    "    assert n_per_sample * n_sample_per_speaker < n_frame, f\"n_frame={n_frame} is too small.\"\n",
    "    train_X = np.swapaxes(train_feats, 1, 2)  # (n_speaker, n_frame, n_mfcc)\n",
    "    train_X = train_X[:, :n_per_sample * n_sample_per_speaker, :]  # discard the rest\n",
    "    train_X = train_X.reshape(-1, n_per_sample * n_mfcc)  # (n_speaker * n_frame / n_per_sample, n_per_sample*n_mfcc)\n",
    "    labels = [speaker for speaker in range(n_speaker) for rep in range(n_sample_per_speaker)]\n",
    "    \n",
    "    valid_feats = preprocess_data(val_speeches)\n",
    "    valid_X = np.swapaxes(valid_feats, 1, 2)\n",
    "    valid_X = valid_X[:, :n_per_sample * n_sample_per_speaker, :]\n",
    "    valid_X = valid_X.reshape(-1, n_per_sample * n_mfcc)\n",
    "\n",
    "    model = MLPClassifier([100])\n",
    "    model.fit(train_X, labels)\n",
    "    pred = model.predict(valid_X)\n",
    "    print(accuracy_score(labels, pred))\n",
    "\n",
    "speaker_identification_experiment3(\n",
    "    dev_speeches[:, :5*SAMPLE_RATE],\n",
    "    dev_speeches[:, 5*SAMPLE_RATE:])  # 4.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 73\n",
      "0.192\n"
     ]
    }
   ],
   "source": [
    "import itertools  \n",
    "\n",
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_mfcc, n_class):\n",
    "        super().__init__()\n",
    "        H = 160\n",
    "        self.model = torch.nn.LSTM(n_mfcc, hidden_size=H, num_layers=3, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(H, n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #h0 = torch.randn(self.model.hidden_size * self.model.num_layers, len(labels), self.model.input_size)\n",
    "        #c0 = torch.randn(self.model.hidden_size * self.model.num_layers, len(labels), self.model.input_size)\n",
    "        \n",
    "        output, (hn, cn) = self.model(torch.from_numpy(X))\n",
    "        # output: (N, L, H_out)\n",
    "\n",
    "        logits = self.fc(output[:, -1, :])\n",
    "        return logits \n",
    "\n",
    "    def fit(self, train_X, labels):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            itertools.chain(self.model.parameters(), self.fc.parameters()),\n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.train()\n",
    "        min_loss = np.inf\n",
    "        for epoch in range(200):\n",
    "            logits = self.forward(train_X)\n",
    "            loss = torch.nn.CrossEntropyLoss()(logits, torch.tensor(labels))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Large batch... all samples into one batch\n",
    "\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss.item()\n",
    "            else:\n",
    "                print (f\"Early stop at epoch {epoch}\")\n",
    "                break \n",
    "\n",
    "    def predict(self, test_X):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(test_X)  # (N, D*H_out)\n",
    "            probs, predictions = logits.max(dim=-1)  # (N)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def speaker_identification_experiment4(train_speeches, val_speeches):\n",
    "    \n",
    "    train_feats = preprocess_data(train_speeches)  # (n_speaker, n_mfcc, n_frame)\n",
    "    n_speaker = train_feats.shape[0]\n",
    "    n_mfcc = train_feats.shape[1]\n",
    "    n_frame = train_feats.shape[2]\n",
    "    n_per_sample = 3\n",
    "    n_sample_per_speaker = 50\n",
    "    assert n_per_sample * n_sample_per_speaker < n_frame, f\"n_frame={n_frame} is too small.\"\n",
    "    train_X = np.swapaxes(train_feats, 1, 2)  # (n_speaker, n_frame, n_mfcc)\n",
    "    train_X = train_X[:, :n_per_sample * n_sample_per_speaker, :]  # discard the rest\n",
    "    train_X = train_X.reshape(-1, n_per_sample, n_mfcc)  # (n_speaker, n_frame / n_per_sample, n_per_sample*n_mfcc)\n",
    "    labels = [speaker for speaker in range(n_speaker) for rep in range(n_sample_per_speaker)]\n",
    "    \n",
    "    valid_feats = preprocess_data(val_speeches)\n",
    "    valid_X = np.swapaxes(valid_feats, 1, 2)\n",
    "    valid_X = valid_X[:, :n_per_sample * n_sample_per_speaker, :]\n",
    "    valid_X = valid_X.reshape(-1, n_per_sample, n_mfcc)\n",
    "\n",
    "    model = LSTMClassifier(n_mfcc, n_speaker)\n",
    "    \n",
    "    model.fit(train_X, labels)\n",
    "    pred = model.predict(valid_X)\n",
    "    print(accuracy_score(labels, pred))\n",
    "\n",
    "speaker_identification_experiment4(\n",
    "    dev_speeches[:, :5*SAMPLE_RATE],\n",
    "    dev_speeches[:, 5*SAMPLE_RATE:])  # 9.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30794021366762253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzi/miniforge3/envs/transformers4/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_speeches = get_audio_signals(\"train-clean-100\")  # (N_speaker, CLIP_TIME * SAMPLE_RATE)\n",
    "speaker_identification_experiment2(\n",
    "    train_speeches[:, :5*SAMPLE_RATE],\n",
    "    train_speeches[:, 5*SAMPLE_RATE:])  # 2 mins, 45 seconds"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c245645368b405f9e41f3dedb59d0df7c5d5feced548513488e8eb3fe8134cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('transformers4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
